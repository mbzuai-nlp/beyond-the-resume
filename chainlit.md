# Welcome to our demo!

**Beyond the Resumé: A Rubric-Aware Automatic Interview System for Information Elicitation**

## Overview

Hi there! Thanks for checking out this proof-of-concept demo of our automatic interview system. You can find our code and data here: [https://github.com/mbzuai-nlp/beyond-the-resume](https://github.com/mbzuai-nlp/beyond-the-resume). When interacting with our demo, you will firstly be asked to upload a resume PDF file. If you are an ACL reviewer, you should have been provided an example file. In any case, feel free to use the example here: [resume download](https://drive.google.com/file/d/1IrUruuZ0eMvx2dSOTxDyEvrxf4Q1c-af/view?usp=sharing). It can take up to 60 seconds for the system to parse the resume file and initialise the *judge*. Once the resume has been processed, you will see the judge's belief over the rubric in a panel that opens on the right. You can hover over each dimension to see the judge's justification of its belief updates. You can also view historical belief states as the interview progresses.

##  Scenario

We invite you to play the role of an applicant applying to the fictitious company, **Big Data Group (BDG).** BDG is hiring for a Machine Learning Engineer (MLE) and have deployed our system to get a better understanding of each applicant. They are looking for a capable MLE using the rubric below.

## Rubric

Each dimension describes the Knowledge, Skills, and Abilities implied by each of the three levels: low, medium, and high.

### Problem framing

- **Low:** Starts work from implementation or model selection without first producing a clear, decision-linked problem statement. Defines success criteria that are vague or not tied to the real decision being improved (e.g., optimizes an offline metric without an argument for online impact). Leaves assumptions, constraints, and operating conditions undocumented, and does not specify evaluation gates or deployment requirements needed for a safe go/no-go decision. Decomposition is shallow: key failure modes, edge cases, and data feasibility checks are not surfaced early and instead emerge reactively during execution.
- **Medium:** Produces a workable problem statement that maps a goal to an ML task with a measurable target and an initial evaluation plan. Identifies major constraints and data requirements, and outlines reasonable baselines and an iterative approach. However, end-to-end alignment is not consistently specified: the plan does not consistently include explicit offline/online alignment arguments, distribution-shift considerations, or long-tail failure modes and guardrails. In ambiguous settings, defaults to convenient formulations/metrics rather than explicitly selecting the most decision-relevant objective and constraints.
- **High:** Produces a crisp, decision-focused framing that explicitly connects the real-world objective to an ML formulation and a deployment context. Documents assumptions and constraints early (including latency, cost, privacy, safety, and maintainability) and defines success metrics plus guardrails that are argued to align offline evaluation with expected online impact. Proactively enumerates key failure modes (slices, drift, abuse cases) and incorporates them into the approach. States what evidence would change the plan and updates the framing as new information arrives while preserving continuity and decision alignment.

### Data judgement

- **Low:** Treats available data as representative by default and proceeds without a documented account of what the dataset represents or how it is generated. Does not routinely check for leakage, shifting definitions, selection effects, or label noise until anomalies appear in results. Chooses labels and features primarily for convenience rather than for fidelity to the deployed system’s inputs, incentives, and measurement process.
- **Medium:** Articulates basic dataset suitability and how labels/features relate to the task, including simple provenance and known limitations. Implements common checks and mitigations for obvious leakage and bias, and proposes practical improvements (sampling changes, cleaning steps, labeling guidelines). However, does not consistently test for subtler mismatches (e.g., proxy-label traps, delayed feedback loops, cohort effects) or specify how data validity will be preserved under deployment dynamics.
- **High:** Evaluates data using an explicit generative account: who/what produces it, under what incentives and constraints, and how the process can change after deployment. Proactively audits leakage paths, proxy-label traps, selection effects, and non-stationarity; designs data collection and labeling to preserve validity (sampling strategy, measurement definitions, auditability). Selects features and labels that remain robust under operational realities and documents dataset limitations in a way that directly shapes modeling, evaluation, and rollout decisions.

### Evaluation design

- **Low:** Uses a single headline metric or default split without an explicit argument that it reflects the desired outcome. Employs evaluation protocols that are vulnerable to leakage or unrepresentative splits (e.g., ignores time or entity boundaries when relevant). Does not routinely assess variance/uncertainty across runs or slices, and does not routinely run calibration checks or subgroup analyses as part of the default evaluation. Does not define or execute targeted checks designed to surface known failure modes early (e.g., key slices, stress cases, or guardrail metrics).
- **Medium:** Defines metrics that are directionally aligned with the goal and uses standard evaluation practices (holdouts, cross-validation where appropriate, baseline comparisons). Includes some robustness checks (e.g., slice breakdowns, calibration checks, multiple seeds) and can interpret tradeoffs. However, the evaluation plan does not consistently mirror deployment conditions (e.g., temporal splits, policy-induced feedback, shifting mixtures), and guardrail metrics and uncertainty criteria are not consistently treated as first-class acceptance/rollback conditions.
- **High:** Designs evaluation to mirror the deployment environment and decision process, including leakage-resistant protocols and splits appropriate to the data-generating process (e.g., time-aware or entity-aware splits when relevant) and realistic mixture assumptions. Incorporates uncertainty-aware reasoning (e.g., variance across runs, confidence intervals or equivalent) and stress-tests claims using targeted ablations, slice analyses, and perturbation/counterfactual-style checks where relevant. Establishes guardrail metrics and explicitly defines acceptance thresholds and conditions that would invalidate the approach or trigger rollback.

### Failure analysis and iteration

- **Low:** Responds to disappointing results primarily with trial-and-error changes (e.g., architecture/hyperparameter tweaking) without isolating variables or forming testable hypotheses. Relies on aggregate metrics without systematic localization (e.g., slice-level errors, pipeline checks, or minimal reproductions). Does not consistently validate fixes across seeds, slices, and reruns, and experiment records are insufficient to reliably reproduce findings or prevent regressions.
- **Medium:** Uses a mix of diagnostic tools (error analysis, ablations, qualitative inspections, and basic pipeline debugging) to identify plausible failure sources and propose targeted interventions. Can reproduce issues and maintains experiment tracking with reasonable hygiene. However, hypotheses and tests are not consistently structured to isolate the causal lever (especially when failures involve interactions across data, model, and system behavior), and validation of fixes across operating conditions is not consistently performed.
- **High:** Runs a hypothesis-driven iteration loop: isolates variables, constructs minimal reproductions, and uses targeted ablations and slice-focused probes to localize root causes. Validates fixes for robustness across seeds, slices, and reruns and articulates why the fix should generalize given observed evidence. Maintains clean experimental records (including datasets, configs, and evaluation protocols) that support fast iteration without accumulating unexamined complexity.

### ML system delivery

- **Low:** Delivers prototypes with unclear interfaces and brittle behavior under edge cases; changes frequently break downstream components. Operational constraints (latency, memory, cost), observability, and safe rollback/disable paths are not designed in from the start. Reliability depends on manual intervention due to missing safeguards, versioning, and predictable runtime behavior.
- **Medium:** Delivers working components that integrate into an existing stack with acceptable reliability. Defines interfaces and includes basic operational safeguards (logging, simple monitors, fallback behavior) and some reproducibility practices. However, resilience to real operating conditions (traffic spikes, schema drift, dependency changes, partial outages) is not consistently designed and tested, and maintainability often degrades as complexity grows due to missing modularization, tests, and interface/version discipline.
- **High:** Delivers ML systems that remain operable and maintainable under realistic constraints: clear interfaces, reproducible builds, versioned artifacts, and predictable performance characteristics. Designs for safe deployment and operation (feature consistency, monitoring tied to anticipated failure modes, rollback/disable mechanisms, and graceful degradation). Anticipates operational risks and reduces them through design choices that keep the system understandable, testable, and resilient over time.

### Communication

- **Low:** Communicates primarily through artifacts (code/results) without clearly stating objectives, decision context, tradeoffs, or implications. Does not consistently document assumptions, risks, uncertainty, and limitations; these are surfaced late. Handoffs omit key context (intent, constraints, operational considerations) and do not specify decision criteria, ownership, or next steps needed to act on the work.
- **Medium:** Explains approach and results clearly to technical audiences and summarizes decisions and tradeoffs for partners in a structured way. Surfaces major risks and limitations and provides actionable next steps. However, alignment is not consistently maintained across iterations: success criteria, decision points, and priority tradeoffs are not always explicitly tracked and refreshed as evidence changes.
- **High:** Maintains durable alignment by explicitly articulating objectives, options, tradeoffs, assumptions, and decision-relevant uncertainty throughout the project. Communicates in a way that enables action: clear recommendations with expected impacts, explicit risks/limitations, and defined next decisions. Proactively updates stakeholders when evidence changes the plan and produces handoff-quality documentation that preserves intent and operational context (interfaces, monitoring/guardrails, rollout/rollback plans) rather than only implementation details.